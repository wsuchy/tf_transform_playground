{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = os.path.join('datasets', 'adult/adult.data')\n",
    "test = os.path.join('datasets', 'adult/adult.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft\n",
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'workclass',\n",
    "    #'education',\n",
    "    #'marital-status',\n",
    "    #'occupation',\n",
    "    #'relationship',\n",
    "    #'race',\n",
    "    #'sex',\n",
    "    #'native-country',\n",
    "]\n",
    "\n",
    "NUMERIC_FEATURE_KEYS = [\n",
    "    'age',\n",
    "    'capital-gain',\n",
    "    #'capital-loss',\n",
    "    #'hours-per-week',\n",
    "]\n",
    "OPTIONAL_NUMERIC_FEATURE_KEYS = [\n",
    "    #'education-num',\n",
    "]\n",
    "LABEL_KEY = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_FEATURE_SPEC = dict(\n",
    "   \n",
    "    [(name, tf.FixedLenFeature([], tf.string))\n",
    "     for name in CATEGORICAL_FEATURE_KEYS] +\n",
    "    [(name, tf.FixedLenFeature([], tf.float32))\n",
    "     for name in NUMERIC_FEATURE_KEYS] +\n",
    "    [(name, tf.VarLenFeature(tf.float32))\n",
    "     for name in OPTIONAL_NUMERIC_FEATURE_KEYS] #+\n",
    "    #[(LABEL_KEY, tf.FixedLenFeature([], tf.string))]\n",
    ")\n",
    "\n",
    "RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(\n",
    "    dataset_schema.from_feature_spec(RAW_DATA_FEATURE_SPEC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = True\n",
    "if testing:\n",
    "  TRAIN_NUM_EPOCHS = 1\n",
    "  NUM_TRAIN_INSTANCES = 1\n",
    "  TRAIN_BATCH_SIZE = 1\n",
    "  NUM_TEST_INSTANCES = 1\n",
    "else:\n",
    "  TRAIN_NUM_EPOCHS = 16\n",
    "  NUM_TRAIN_INSTANCES = 32561\n",
    "  TRAIN_BATCH_SIZE = 128\n",
    "  NUM_TEST_INSTANCES = 16281\n",
    "\n",
    "# Names of temp files\n",
    "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
    "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
    "EXPORTED_MODEL_DIR = 'exported_model_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapAndFilterErrors(beam.PTransform):\n",
    "  \"\"\"Like beam.Map but filters out erros in the map_fn.\"\"\"\n",
    "\n",
    "  class _MapAndFilterErrorsDoFn(beam.DoFn):\n",
    "    \"\"\"Count the bad examples using a beam metric.\"\"\"\n",
    "\n",
    "    def __init__(self, fn):\n",
    "      self._fn = fn\n",
    "      # Create a counter to measure number of bad elements.\n",
    "      self._bad_elements_counter = beam.metrics.Metrics.counter(\n",
    "          'census_example', 'bad_elements')\n",
    "\n",
    "    def process(self, element):\n",
    "      try:\n",
    "        yield self._fn(element)\n",
    "      except Exception:  # pylint: disable=broad-except\n",
    "        # Catch any exception the above call.\n",
    "        self._bad_elements_counter.inc(1)\n",
    "\n",
    "  def __init__(self, fn):\n",
    "    self._fn = fn\n",
    "\n",
    "  def expand(self, pcoll):\n",
    "    return pcoll | beam.ParDo(self._MapAndFilterErrorsDoFn(self._fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    # Since we are modifying some features and leaving others unchanged, we\n",
    "    # start by setting `outputs` to a copy of `inputs.\n",
    "    outputs = inputs.copy()\n",
    "    res = dict()\n",
    "    # Scale numeric columns to have range [0, 1].\n",
    "    for key in NUMERIC_FEATURE_KEYS:\n",
    "        res[key] = tft.scale_to_0_1(outputs[key], name=key)\n",
    "\n",
    "    for key in OPTIONAL_NUMERIC_FEATURE_KEYS:\n",
    "    # This is a SparseTensor because it is optional. Here we fill in a default\n",
    "    # value when it is missing.\n",
    "        dense = tf.sparse_to_dense(outputs[key].indices,\n",
    "                                   [outputs[key].dense_shape[0], 1],\n",
    "                                   outputs[key].values, default_value=0.)\n",
    "        # Reshaping from a batch of vectors of size 1 to a batch to scalars.\n",
    "        dense = tf.squeeze(dense, axis=1)\n",
    "        res[key] = tft.scale_to_0_1(dense, name = key)\n",
    "\n",
    "\n",
    "    # For all categorical columns except the label column, we generate a\n",
    "    # vocabulary but do not modify the feature.  This vocabulary is instead\n",
    "    # used in the trainer, by means of a feature column, to convert the feature\n",
    "    # from a string to an integer id.\n",
    "    for key in CATEGORICAL_FEATURE_KEYS:\n",
    "        v1 = tft.compute_and_apply_vocabulary(\n",
    "            inputs[key],\n",
    "            default_value=-1,\n",
    "            top_k=100,\n",
    "            frequency_threshold=0.05,\n",
    "            num_oov_buckets=0,\n",
    "            vocab_filename=key,\n",
    "            weights=None,\n",
    "            labels=None,\n",
    "            use_adjusted_mutual_info=False,\n",
    "            min_diff_from_avg=0.0,\n",
    "            coverage_top_k=None,\n",
    "            coverage_frequency_threshold=None,\n",
    "            key_fn=None,\n",
    "\n",
    "            name=key\n",
    "        )\n",
    "        \n",
    "        res[key] = tf.cast(v1,tf.float32)\n",
    "    \n",
    "\n",
    "    return {\"XXALLXX\":tf.stack([res[o] for o in res],axis=1, name=\"output_all\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_file, test_data_file, working_dir):\n",
    "  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
    "\n",
    "  Read in the data using the CSV reader, and transform it using a\n",
    "  preprocessing pipeline that scales numeric data and converts categorical data\n",
    "  from strings to int64 values indices, by creating a vocabulary for each\n",
    "  category.\n",
    "\n",
    "  Args:\n",
    "    train_data_file: File containing training data\n",
    "    test_data_file: File containing test data\n",
    "    working_dir: Directory to write transformed data and metadata to\n",
    "  \"\"\"\n",
    "\n",
    "  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
    "  # of the block.\n",
    "  with beam.Pipeline() as pipeline:\n",
    "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "      # Create a coder to read the census data with the schema.  To do this we\n",
    "      # need to list all columns in order since the schema doesn't specify the\n",
    "      # order of columns in the csv.\n",
    "      ordered_columns = [\n",
    "          'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "          'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "          'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "          'label'\n",
    "      ]\n",
    "      converter = tft.coders.CsvCoder(ordered_columns, RAW_DATA_METADATA.schema)\n",
    "\n",
    "      # Read in raw data and convert using CSV converter.  Note that we apply\n",
    "      # some Beam transformations here, which will not be encoded in the TF\n",
    "      # graph since we don't do the from within tf.Transform's methods\n",
    "      # (AnalyzeDataset, TransformDataset etc.).  These transformations are just\n",
    "      # to get data into a format that the CSV converter can read, in particular\n",
    "      # removing spaces after commas.\n",
    "      #\n",
    "      # We use MapAndFilterErrors instead of Map to filter out decode errors in\n",
    "      # convert.decode which should only occur for the trailing blank line.\n",
    "      raw_data = (\n",
    "          pipeline\n",
    "          | 'ReadTrainData' >> beam.io.ReadFromText(train_data_file)\n",
    "          | 'FixCommasTrainData' >> beam.Map(\n",
    "              lambda line: line.replace(', ', ','))\n",
    "          | 'DecodeTrainData' >> MapAndFilterErrors(converter.decode))\n",
    "\n",
    "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
    "      # the schema to read the CSV data, but we also need it to interpret\n",
    "      # raw_data.\n",
    "      raw_dataset = (raw_data, RAW_DATA_METADATA)\n",
    "      \n",
    "          \n",
    "      transform_fn =  raw_dataset |  tft_beam.AnalyzeDataset(preprocessing_fn)\n",
    "\n",
    "      print(\"workdir\",working_dir )\n",
    "      #print(transform_fn)\n",
    "      _ = (\n",
    "          transform_fn\n",
    "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))\n",
    "      return transform_fn\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "transform_data(train, test, \"/tmp/tft10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now try to inspect /tmp/tft10/transform_fn/saved_model.pb file with vim. You will find hardcoded path like:**   \n",
    "`/var/folders/h3/zt26x1d93hq49vff996w04pc0000gq/T/tmpndp56yw3/tftransform_tmp/workclass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "from tensorflow.python.platform import gfile\n",
    "tf.reset_default_graph()\n",
    "from tensorflow.core.protobuf import saved_model_pb2\n",
    "from tensorflow.python.util import compat\n",
    "import tensorflow.contrib as tfc\n",
    "with tf.Session() as sess:\n",
    "    model_filename ='/tmp/tft10/transform_fn/saved_model.pb'\n",
    "    with gfile.FastGFile(model_filename, 'rb') as f:\n",
    "\n",
    "        data = compat.as_bytes(f.read())\n",
    "        sm = saved_model_pb2.SavedModel()\n",
    "        sm.ParseFromString(data)\n",
    "\n",
    "        if 1 != len(sm.meta_graphs):\n",
    "            print('More than one graph found. Not sure which to write')\n",
    "            sys.exit(1)\n",
    "\n",
    "        \n",
    "        g_in = tf.import_graph_def(sm.meta_graphs[0].graph_def)\n",
    "       \n",
    "    \n",
    "        opName = \"import/transform/workclass/apply_vocab/string_to_index/hash_table/table_init/InitializeTableFromTextFileV2\"\n",
    "        # this is the only way I was able to init the hash table\n",
    "        sess.run( sess.graph.get_operation_by_name(opName))\n",
    "        #that doesn't work:\n",
    "        #sess.run(tf.initializers.tables_initializer(name='initialize_all_tables'))\n",
    "        in1 = sess.graph.get_tensor_by_name(\"import/transform/inputs/age:0\")\n",
    "      \n",
    "        out1 = sess.graph.get_tensor_by_name(\"import/transform/output_all:0\")\n",
    "        \n",
    "        in2 = sess.graph.get_tensor_by_name(\"import/transform/inputs/capital-gain:0\")\n",
    "        in3 = sess.graph.get_tensor_by_name(\"import/transform/inputs/workclass:0\")\n",
    "      \n",
    "        # uncomment this to see all nodes incl. hardcoded path to workclass file\n",
    "        #for o in sm.meta_graphs[0].graph_def.node:\n",
    "        #    print(o)\n",
    "        res = sess.run([out1], feed_dict = {in1: [35,60], in2:[100000, 80000], in3:[\"Local-gov\",\"bbbb\"]})\n",
    "        print(res)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.graph_util import extract_sub_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph](graph_scale_0_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
